{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with RNN\n",
    "\n",
    "Implementing an RNN that performs Sentiment Analysis I am using a dataset of movie reviews, and a dataset of sentiment labels: positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "with open('/kaggle/input/reviewslabels/reviews.txt','r') as f:\n",
    "    reviews = f.read()\n",
    "with open('/kaggle/input/reviewslabels/labels.txt','r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n",
      "homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if y\n"
     ]
    }
   ],
   "source": [
    "print(reviews[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the']\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "#remove punctuations\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "reviews = reviews.lower()\n",
    "\n",
    "#remove punctuations\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "\n",
    "reviews_split = all_text.split('\\n')\n",
    "\n",
    "all_text = ' '.join(reviews_split)\n",
    "\n",
    "# print(all_text[:2000])\n",
    "# print(type(reviews_split))\n",
    "\n",
    "words = all_text.split()\n",
    "\n",
    "print(words[:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#build a dictionary of words and counts\n",
    "counts = Counter(words)\n",
    "\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "vocab_to_int = {c:i for i,c in enumerate(vocab,1)}\n",
    "\n",
    "\n",
    "\n",
    "#store the reviews as a list\n",
    "\n",
    "reviews_int = []\n",
    "\n",
    "for review  in reviews_split:\n",
    "    reviews_int.append([vocab_to_int[c] for c in review.split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74072\n",
      "[[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23], [63, 4, 3, 125, 36, 47, 7472, 1395, 16, 3, 4181, 505, 45, 17, 3, 622, 134, 12, 6, 3, 1279, 457, 4, 1721, 207, 3, 10624, 7373, 300, 6, 667, 83, 35, 2116, 1086, 2989, 34, 1, 898, 46417, 4, 8, 13, 5096, 464, 8, 2656, 1721, 1, 221, 57, 17, 58, 794, 1297, 832, 228, 8, 43, 98, 123, 1469, 59, 147, 38, 1, 963, 142, 29, 667, 123, 1, 13584, 410, 61, 94, 1774, 306, 755, 5, 3, 819, 10396, 22, 3, 1724, 635, 8, 13, 128, 73, 21, 233, 102, 17, 49, 50, 617, 34, 682, 85, 28785, 28786, 682, 374, 3341, 11398, 2, 16371, 7946, 51, 29, 108, 3324]]\n"
     ]
    }
   ],
   "source": [
    "# testing the result\n",
    "\n",
    "#unique words\n",
    "print(len(vocab_to_int))\n",
    "\n",
    "#print tokenized review\n",
    "\n",
    "print(reviews_int[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode labels to 0 and 1\n",
    "\n",
    "labels_split = labels.split('\\n')\n",
    "encoded_labels = np.array([1 if x == 'positive' else 0 for x in labels_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n",
      "Avg review length: 539.1170731707317\n",
      "Median review length: 524\n"
     ]
    }
   ],
   "source": [
    "#preprocessing\n",
    "\n",
    "review_len = Counter([len(x) for x in reviews_int])\n",
    "\n",
    "print(\"Zero-length reviews: {}\".format(review_len[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_len)))\n",
    "print(\"Avg review length: {}\".format(sum(review_len)/len(review_len)))\n",
    "print(\"Median review length: {}\".format(sorted(review_len)[int(len(review_len)/2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of reviews before: 25001\n",
      "num of reviews after: 25000\n"
     ]
    }
   ],
   "source": [
    "# removing the zero length reviews\n",
    "\n",
    "print('num of reviews before:', len(reviews_int))\n",
    "\n",
    "non_zero_idx = [i for i,x in enumerate(reviews_int) if len(reviews_int[i]) != 0 ]\n",
    "\n",
    "reviews_int = [reviews_int[i] for i in  non_zero_idx]\n",
    "\n",
    "encoded_labels = np.array([encoded_labels[i] for i in non_zero_idx])\n",
    "\n",
    "print ('num of reviews after:', len(reviews_int))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make the reviews same length\n",
    "# shorter reviews are left padded with zeros\n",
    "# this will make the reviews_int a 2d array of rows = num of reviews and cols = seq_length\n",
    "\n",
    "def pad_features(reviews_int, seq_len):\n",
    "    \n",
    "    features = np.zeros((len(reviews_int), seq_len),dtype=int)\n",
    "    \n",
    "    for i,x in enumerate(reviews_int):\n",
    "        features[i,-len(x):] = np.array(x)[:seq_len]\n",
    "    \n",
    "    return features\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 200\n",
    "\n",
    "features = pad_features(reviews_int, seq_len=seq_length)\n",
    "\n",
    "assert len(features) == len (reviews_int), \" The number of rows is different\"\n",
    "assert features.shape[1] == seq_length, \" The number of columns is different\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
      " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   54    10    14   116    60   798   552    71   364     5]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    1   330   578    34     3   162   748  2731     9   325]\n",
      " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
      " [    1    20     6    76    40     6    58    81    95     5]]\n"
     ]
    }
   ],
   "source": [
    "print(features [:20, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: (20000, 200) label (20000,) \n",
      "test set: (2500, 200) label (2500,) \n",
      "val set: (2500, 200) label (2500,) \n"
     ]
    }
   ],
   "source": [
    "# spliting the data to training test and validation sets\n",
    "split_frac = 0.8\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "\n",
    "train_x, remaining_x = features[:split_idx],features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "\n",
    "split_idx = int(len(remaining_x)*0.5)\n",
    "\n",
    "val_x, test_x = remaining_x[:split_idx], remaining_x[split_idx:]\n",
    "val_y, test_y = remaining_y[:split_idx], remaining_y[split_idx:]\n",
    "\n",
    "\n",
    "print(\"training set: {} label {} \".format (train_x.shape, train_y.shape))\n",
    "print(\"test set: {} label {} \".format (test_x.shape, test_y.shape))\n",
    "print(\"val set: {} label {} \".format (val_x.shape, val_y.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading  the data to Dataloader and batching\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x),torch.from_numpy(train_y))\n",
    "test_data  = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "val_data = TensorDataset(torch.from_numpy(val_x),torch.from_numpy(val_y))\n",
    "\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle = True, batch_size= batch_size)\n",
    "test_loader = DataLoader (test_data, shuffle = True, batch_size = batch_size)\n",
    "val_loader = DataLoader (val_data, shuffle=True, batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 200])\n",
      "torch.Size([50])\n",
      "tensor([[    1,   341,     4,  ...,    75,     5,   122],\n",
      "        [   11,    20,     6,  ...,   470, 14115,   427],\n",
      "        [ 1815,    13,   340,  ...,    14,   169,     5],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,   210,    42,   159],\n",
      "        [    1,  2797,  2912,  ...,    22,  1411,    13],\n",
      "        [ 1025,    60,   573,  ...,   109,  3086,    19]])\n",
      "tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 1])\n"
     ]
    }
   ],
   "source": [
    "#testing \n",
    "\n",
    "jangiter = iter(train_loader)\n",
    "\n",
    "jangx, jangy = next(jangiter)\n",
    "\n",
    "print(jangx.size())\n",
    "print (jangy.size())\n",
    "\n",
    "print(jangx)\n",
    "print (jangy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if train_on_gpu:\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available.Using CPU\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,n_layers,dropout=drop_prob, batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self,x, hidden):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(embeds,hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1,self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        #reshape to batch_size first\n",
    "        sig_out = sig_out.view (batch_size, -1)\n",
    "      \n",
    "        sig_out = sig_out[:, -1] # get last batch of labels, last label will say if it is +ve or -ve\n",
    "        \n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(74073, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#define hyper parameters and intantiate the model \n",
    "\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining training hyper parameters\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 100... Loss: 0.489845... Val Loss: 0.586288\n",
      "Epoch: 1/4... Step: 200... Loss: 0.563123... Val Loss: 0.579647\n",
      "Epoch: 1/4... Step: 300... Loss: 0.432164... Val Loss: 0.499122\n",
      "Epoch: 1/4... Step: 400... Loss: 0.599542... Val Loss: 0.497215\n",
      "Epoch: 2/4... Step: 500... Loss: 0.401204... Val Loss: 0.496560\n",
      "Epoch: 2/4... Step: 600... Loss: 0.345174... Val Loss: 0.458867\n",
      "Epoch: 2/4... Step: 700... Loss: 0.451744... Val Loss: 0.525779\n",
      "Epoch: 2/4... Step: 800... Loss: 0.302977... Val Loss: 0.451224\n",
      "Epoch: 3/4... Step: 900... Loss: 0.248862... Val Loss: 0.485913\n",
      "Epoch: 3/4... Step: 1000... Loss: 0.176518... Val Loss: 0.467932\n",
      "Epoch: 3/4... Step: 1100... Loss: 0.364825... Val Loss: 0.469616\n",
      "Epoch: 3/4... Step: 1200... Loss: 0.238338... Val Loss: 0.492017\n",
      "Epoch: 4/4... Step: 1300... Loss: 0.153911... Val Loss: 0.553701\n",
      "Epoch: 4/4... Step: 1400... Loss: 0.150469... Val Loss: 0.511068\n",
      "Epoch: 4/4... Step: 1500... Loss: 0.120247... Val Loss: 0.512971\n",
      "Epoch: 4/4... Step: 1600... Loss: 0.397564... Val Loss: 0.519052\n"
     ]
    }
   ],
   "source": [
    "epochs = 4 \n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip = 5 \n",
    "\n",
    "if train_on_gpu:\n",
    "    net=net.cuda()\n",
    "\n",
    "net.train()\n",
    "\n",
    "for e in range(epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter +=1 \n",
    "        \n",
    "        if train_on_gpu:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                        \n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        net.zero_grad()\n",
    "        \n",
    "        output, h = net(inputs, h)\n",
    "        \n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()    \n",
    "        \n",
    "        #validation \n",
    "        if counter % print_every == 0:\n",
    "            \n",
    "            \n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            net.eval()\n",
    "            val_losses = []\n",
    "            \n",
    "            for inputs, labels in val_loader:\n",
    "                \n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                \n",
    "                if train_on_gpu:\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                    \n",
    "                output, val_h = net(inputs,val_h)\n",
    "                \n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "                \n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            net.train()\n",
    "            \n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))       \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.524\n",
      "Test accuracy: 0.800\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    " \n",
    "for inputs, labels in test_loader:\n",
    "     \n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if train_on_gpu:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on a test review¶\n",
    "\n",
    "Giving a user given random review and testing the model for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n",
    "# test_review_neg = 'I wasted my time '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 247, 18, 10, 28, 108, 113, 14, 388, 2, 10, 181, 60, 273, 144, 11, 18, 68, 76, 113, 2, 1, 410, 14, 539]]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def tokenize_review(test_review):\n",
    "    test_review = test_review.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "# test code and generate tokenized review\n",
    "test_ints = tokenize_review(test_review_neg)\n",
    "print(test_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   1 247  18  10  28\n",
      "  108 113  14 388   2  10 181  60 273 144  11  18  68  76 113   2   1 410\n",
      "   14 539]]\n"
     ]
    }
   ],
   "source": [
    "# test sequence padding\n",
    "seq_length=200\n",
    "features = pad_features(test_ints, seq_length)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "# test conversion to tensor and pass into your model\n",
    "feature_tensor = torch.from_numpy(features)\n",
    "print(feature_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_review, sequence_length=200):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"Positive review detected!\")\n",
    "    else:\n",
    "        print(\"Negative review detected.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.001802\n",
      "Negative review detected.\n"
     ]
    }
   ],
   "source": [
    "# call function\n",
    "seq_length=200 # good to use the length that was trained on\n",
    "\n",
    "predict(net, test_review_neg, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive test review\n",
    "# test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n",
    "\n",
    "# test_review_pos = 'this is an awesome movie'\n",
    "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.987574\n",
      "Positive review detected!\n"
     ]
    }
   ],
   "source": [
    "# call function\n",
    "seq_length=200 # good to use the length that was trained on\n",
    "\n",
    "predict(net, test_review_pos, seq_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
